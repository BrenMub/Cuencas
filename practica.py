# -*- coding: utf-8 -*-
"""practica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JuPqMysNeFb59zJMwQU7ZXqzZGz8figV
"""

from google.colab import files
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.optimizers import SGD
from tensorflow.keras.models import load_model
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.layers import Dense, Dropout

uploaded = files.upload()

df = pd.read_excel("basemensual1.xlsx")

print(df.head())

lag = 2
for i in range(1, lag + 1):
    df[f'chirps_lag{i}'] = df['chirps'].shift(i)
    df[f'chirts_lag{i}'] = df['chirts'].shift(i)
    df[f'caudal_lag{i}'] = df['M3/SEG_2'].shift(i)



lagc = 1
for i in range(1, lagc + 1):
    df[f'NINO3.4_lagc{i}'] = df['NINO3.4'].shift(i)

df = df.dropna()

scaler = MinMaxScaler()
features = ([f'{var}_lag{i}' for var in ['chirps', 'chirts', 'caudal'] for i in range(1, lag + 1)]+    [f'NINO3.4_lagc{i}' for i in range(1, lagc + 1)] # Rezagos
)
df[features] = scaler.fit_transform(df[features])

train_size = int(len(df) * 0.8)
train = df.iloc[:train_size]
test = df.iloc[train_size:]

X_train = train[features]
y_train = train['M3/SEG_2']
X_test = test[features]
y_test = test['M3/SEG_2']

X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1))


optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss='mse', optimizer=optimizer)

# Early stopping para evitar sobreajuste
early_stop = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)


history = model.fit(
    X_train_final, y_train_final,
    validation_data=(X_val, y_val),
    epochs=800,
    batch_size=50,
    callbacks=[early_stop],
    verbose=1
)


y_pred = model.predict(X_test).flatten()

# Métricas
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Métricas del modelo MLP calibrado:")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R²: {r2}")

plt.figure(figsize=(12, 6))
plt.plot(test['date'], y_test, label="Valores reales")
plt.plot(test['date'], y_pred, label="Predicciones", linestyle="--")
plt.xlabel("Fecha")
plt.ylabel("M3/SEG_2")
plt.legend()
plt.title("Predicción de Caudal con MLP Calibrado")
plt.show()

model.save('mi_modelo_mlp.h5')

model = load_model('mi_modelo_mlp.h5', custom_objects={'mse': MeanSquaredError()})

train_size = int(len(df) * 0.8)
train = df.iloc[:train_size]
test = df.iloc[train_size:]

X_test = test[features]
y_test = test['M3/SEG_2']

y_pred = model.predict(X_test).flatten()

plt.figure(figsize=(12, 6))
plt.plot(test.index, y_test, label="Valores reales")
plt.plot(test.index, y_pred, label="Predicciones", linestyle="--")
plt.xlabel("date")
plt.ylabel("M3/SEG_2")
plt.legend()
plt.title("Predicción de Caudal con MLP Calibrado")
plt.show()

"""Tomando en cuenta las variables sin rezagar"""

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Métricas del modelo MLP calibrado:")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")
print(f"R²: {r2}")

from sklearn.inspection import permutation_importance
from tensorflow.keras.models import load_model

perm_importance = permutation_importance(model, X_test, y_test, scoring='neg_mean_squared_error', n_repeats=10, random_state=42)


feature_importances = pd.DataFrame({
    'feature': features,
    'importance': perm_importance.importances_mean
})

feature_importances = feature_importances.sort_values(by='importance', ascending=False)

print("Importancia de las características:")
print(feature_importances)

"""#Datos diarios"""

uploaded = files.upload()

data = pd.read_excel("basedatosdiarioo.xlsx")

print(data.head())

plt.figure(figsize=(12, 6))
    plt.plot(data['caudal'], label='Caudal', color='blue')
    plt.title('Gráfico Lineal de Caudal')
    plt.xlabel('Índice de Tiempo')
    plt.ylabel('Caudal')
    plt.legend()
    plt.grid(True)
    plt.show()

lag = 6
for i in range(1, lag + 1):
    data[f'chirps_lag{i}'] = data['chirps'].shift(i)
    data[f'chirts_lag{i}'] = data['chirts'].shift(i)
    data[f'Caudal_lag{i}'] = data['caudal'].shift(i)
    ##df[f'NINO3.4_lag{i}'] = df['NINO3.4'].shift(i)


#lagc = 1
#for i in range(1, lagc + 1):
 #   df[f'caudal_lagc{i}'] = df['M3/SEG_2'].shift(i)
 #   df[f'NINO3.4_lagc{i}'] = df['NINO3.4'].shift(i)

data = data.dropna()

data

from sklearn.inspection import permutation_importance
import numpy as np
import pandas as pd
import tensorflow as tf


def keras_predict(model, X):

    return model.predict(X).flatten()

perm_importance = permutation_importance(
    model,
    X_test_scaled,
    y_test_scaled,
    scoring='neg_mean_squared_error',
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)


feature_importances = pd.DataFrame({
    'feature': X_test.columns,
    'importance': perm_importance.importances_mean
})


feature_importances = feature_importances.sort_values(by='importance', ascending=False)

print("Importancia de las características:")
print(feature_importances)

train_data = data[(data['datetime'] < '1987-01-01')]
test_data = data[(data['datetime'] >= '1987-01-01')]


X_train = train_data[['Caudal_lag1', 'Caudal_lag2', 'Caudal_lag3',
                      'chirps_lag1','chirps_lag2','chirps_lag3','chirps_lag4',
                      'chirts_lag1','chirts_lag2','chirts_lag3','chirts_lag4','chirts_lag5','chirts_lag6']]
y_train = train_data['caudal']

X_test = test_data[['Caudal_lag1', 'Caudal_lag2', 'Caudal_lag3',
                      'chirps_lag1','chirps_lag2','chirps_lag3','chirps_lag4',
                      'chirts_lag1','chirts_lag2','chirts_lag3','chirts_lag4','chirts_lag5','chirts_lag6']]
y_test = test_data['caudal']


scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))

# 5. Crear la red neuronal
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dropout(0.3))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)


history = model.fit(X_train_scaled, y_train_scaled,
                    epochs=150, batch_size=32, validation_split=0.2, verbose=1,
                    callbacks=[early_stopping])

predictions_scaled = model.predict(X_test_scaled)
predictions = scaler_y.inverse_transform(predictions_scaled).flatten()

train_data['DiaMes'] = train_data['datetime'].dt.strftime('%m-%d')
test_data['DiaMes'] = test_data['datetime'].dt.strftime('%m-%d')


y_test_grouped = pd.DataFrame({'DiaMes': test_data['DiaMes'], 'Real': y_test}).groupby('DiaMes').mean()
predictions_grouped = pd.DataFrame({'DiaMes': test_data['DiaMes'], 'Prediccion': predictions}).groupby('DiaMes').mean()


resultados = y_test_grouped.merge(predictions_grouped, left_index=True, right_index=True)



from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_test_pred = predictions

# Evaluar métricas
rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)


print(f"R²: {r2}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")

resultados['Fecha'] = pd.to_datetime(resultados.index, format='%m-%d')

plt.figure(figsize=(12, 6))
plt.plot(resultados['Fecha'], resultados['Real'], label='Datos reales', color='blue')
plt.plot(resultados['Fecha'], resultados['Prediccion'], label='Predicciones', color='orange', linestyle='dashed')


plt.xticks(ticks=resultados['Fecha'][::31], labels=resultados['Fecha'][::31].dt.month)


plt.title('Predicción del Caudal Diario con Etiquetas de Meses')
plt.xlabel('Mes')
plt.ylabel('Caudal')
plt.legend()
plt.grid(True)
plt.show()

# Graficar la historia del entrenamiento
plt.plot(history.history['loss'], label='Pérdida de entrenamiento')
plt.plot(history.history['val_loss'], label='Pérdida de validación')
plt.title('Pérdida durante el entrenamiento')
plt.xlabel('Épocas')
plt.ylabel('Pérdida')
plt.legend()
plt.show()

"""#considerando solo la epoca seca"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

data['Mes'] = data['datetime'].dt.month
epoca_seca = data[(data['Mes'] >= 12) | (data['Mes'] <= 4)]

test_data = epoca_seca[((epoca_seca['datetime'] >= '1986-12-01') & (epoca_seca['datetime'] <= '1987-04-30'))]


epoca_seca = epoca_seca[~((epoca_seca['datetime'] >= '1987-12-01') & (epoca_seca['datetime'] <= '1987-12-31'))]


train_data = epoca_seca[epoca_seca['datetime'] < '1986-12-01']


X_train = train_data[['Caudal_lag1', 'Caudal_lag2', 'Caudal_lag3',
                      'chirps_lag1','chirps_lag2','chirps_lag3','chirps_lag4',
                      'chirts_lag1','chirts_lag2','chirts_lag3','chirts_lag4','chirts_lag5','chirts_lag6']]
y_train = train_data['caudal']

X_test = test_data[['Caudal_lag1', 'Caudal_lag2', 'Caudal_lag3',
                      'chirps_lag1','chirps_lag2','chirps_lag3','chirps_lag4',
                      'chirts_lag1','chirts_lag2','chirts_lag3','chirts_lag4','chirts_lag5','chirts_lag6']]
y_test = test_data['caudal']


scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))

model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)))
model.add(Dropout(0.3))  # Regularización por Dropout
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))  # Regularización por Dropout
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))  # Regularización por Dropout
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

#Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)


history = model.fit(X_train_scaled, y_train_scaled,
                    epochs=150, batch_size=32, validation_split=0.2, verbose=1,
                    callbacks=[early_stopping])

predictions_scaled = model.predict(X_test_scaled)
predictions = scaler_y.inverse_transform(predictions_scaled).flatten()

train_data['DiaMes'] = train_data['datetime'].dt.strftime('%m-%d-%y')
test_data['DiaMes'] = test_data['datetime'].dt.strftime('%m-%d-%y')


y_test_grouped = pd.DataFrame({'DiaMes': test_data['DiaMes'], 'Real': y_test}).groupby('DiaMes').mean()
predictions_grouped = pd.DataFrame({'DiaMes': test_data['DiaMes'], 'Prediccion': predictions}).groupby('DiaMes').mean()


resultados = y_test_grouped.merge(predictions_grouped, left_index=True, right_index=True)


from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


y_test_pred = predictions


rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)


print(f"R²: {r2}")
print(f"RMSE: {rmse}")
print(f"MAE: {mae}")


resultados['Fecha'] = pd.to_datetime(resultados.index, format='%m-%d-%y')

resultados = resultados.sort_values(by='Fecha')

plt.figure(figsize=(12, 6))


plt.plot(resultados['Fecha'], resultados['Real'], label='Datos reales', color='blue')
plt.plot(resultados['Fecha'], resultados['Prediccion'], label='Predicciones', color='gray', linestyle='dashed')


plt.xticks(ticks=resultados['Fecha'][::31], labels=resultados['Fecha'][::31].dt.month)


plt.grid(True)

plt.title('Predicción del Caudal en Época Seca (Diciembre 1986 - Abril 1987)')
plt.xlabel('Mes')
plt.ylabel('Caudal')
plt.legend()
plt.show()